{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142364c6-14a7-4540-89cb-845812f23f6c",
   "metadata": {},
   "source": [
    "# Part 2: RAG\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f7678-dd7b-48bf-8bc1-f81b6c52f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langgraph.graph import START, StateGraph\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "import litellm\n",
    "from litellm import completion\n",
    "import instructor\n",
    "from instructor import Mode\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fff4d-25c2-4b78-a4f1-634fea386b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = \"Kmvh0N6KGE3Rq2eJtOSZOgA_0n3oEUEZhbqd5w0fyGRd\"\n",
    "PROJECT_ID = \"d0c9b183-186c-4eaf-96dc-d8e4285fe71b\"\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5811af8-8cd3-4e19-8d01-a422c6239e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "\n",
    "        model_id= \"ibm/granite-13b-instruct-v2\",\n",
    "        url=WX_API_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=PROJECT_ID,\n",
    "\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 5,\n",
    "            GenParams.MAX_NEW_TOKENS: 1_000,\n",
    "            GenParams.REPETITION_PENALTY:1.2\n",
    "        }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63610e-8e66-47ca-bdc6-315f49b6bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"Hi how are you?\"])\n",
    "\n",
    "print(type(llm_result))\n",
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66c040-298e-463a-bf18-e31efd3b22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().cwd()\n",
    "\n",
    "desc_dir      = project_root / \"parsed_markdown\"\n",
    "output_file   = project_root / \"parsed_markdown\" / \"consolidated_markdown_data.md\"\n",
    "\n",
    "md_files = sorted(desc_dir.glob(\"*.md\"))\n",
    "\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as out:\n",
    "    for md in md_files:\n",
    "        out.write(f\"<!-- ===== {md.name} ===== -->\\n\\n\")\n",
    "        \n",
    "        out.write(md.read_text(encoding=\"utf-8\"))\n",
    "        out.write(\"\\n\\n\")  # add a blank line between files\n",
    "\n",
    "print(f\" Consolidated {len(md_files)} files into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6cdb3-4640-462a-b2bc-2175d2eb3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "project_root = Path().cwd()\n",
    "file_path    = project_root / \"parsed_markdown\" / \"consolidated_markdown_data.md\"\n",
    "\n",
    "loader    = TextLoader(str(file_path))\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "document  = documents[0]\n",
    "print(document .metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c369ed-243b-4063-b89f-9d28b3b26a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410d210-bca1-486e-84f5-65569ea3be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Structural split by headers\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "header_chunks = markdown_splitter.split_text(document.page_content)\n",
    "\n",
    "# Step 2: Token-based split inside each header chunk\n",
    "token_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Step 3: Apply token-based split to each chunk\n",
    "final_chunks = []\n",
    "for chunk in header_chunks:\n",
    "    sub_chunks = token_splitter.split_text(chunk.page_content)\n",
    "    for sub_chunk in sub_chunks:\n",
    "        final_chunks.append(Document(page_content=sub_chunk, metadata=chunk.metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ebffe-1568-4e64-9072-c99373ec46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chunks = []\n",
    "for chunk in final_chunks:\n",
    "    is_footer = chunk.metadata.get(\"Header 3\") == \"Page Footer\"\n",
    "    is_mostly_link_or_boilerplate = (\n",
    "        len(chunk.page_content.strip()) < 150 or  # too short\n",
    "        chunk.page_content.count(\"http\") > 1      # mostly links\n",
    "    )\n",
    "\n",
    "    if is_footer and is_mostly_link_or_boilerplate:\n",
    "        continue  # Skip useless footers\n",
    "    filtered_chunks.append(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a945fd3-a375-4fdf-8e9f-77c7d0ca35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(final_chunks):\n",
    "    if \"574,99 â‚¬\" in chunk.page_content:\n",
    "        print(f\"FOUND in chunk {i}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c8bb5-dd41-49e6-966f-701fb33ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519eb478-593c-43f4-84e1-6f49731b8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def update_documents_with_headers(chunks):\n",
    "    \"\"\"\n",
    "    Creates a new list of Document objects with page_content prepended with headers\n",
    "    in [Header1/Header2/Header3]: format\n",
    "    \n",
    "    Returns new objects rather than modifying the original chunks\n",
    "    \"\"\"\n",
    "    updated_chunks = []\n",
    "    max_depth=3 \n",
    "    \n",
    "    for doc in chunks:\n",
    "        # Create a deep copy of the document to avoid modifying the original\n",
    "        new_doc = deepcopy(doc)\n",
    "        \n",
    "        # Get all headers that exist in metadata\n",
    "        headers = []\n",
    "        for i in range(1, max_depth + 1):\n",
    "            key = f'Header {i}'\n",
    "            if key in new_doc.metadata:\n",
    "                headers.append(f\"{key}: {new_doc.metadata[key]}\")\n",
    "        \n",
    "        # Create the header prefix and update page_content\n",
    "        if headers:\n",
    "            prefix = f\"[{'/'.join(headers)}]: \"\n",
    "            new_doc.page_content = prefix + \"\\n\" + new_doc.page_content\n",
    "        \n",
    "        updated_chunks.append(new_doc)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "\n",
    "docs = update_documents_with_headers(filtered_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12637c-59ed-4c00-be74-6beef1a9a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "product_names = [\n",
    " 'K 7 Premium Smart Control Flex eco!B',\n",
    " 'K 7 Premium Smart Control Flex Home',\n",
    " 'K 7 Premium Power Flex',\n",
    " 'K 7 Premium Power Flex Home',\n",
    " 'K 7 Premium Smart Control Flex',\n",
    " 'K 7 Smart Control Flex eco!Booster',\n",
    " 'K 7 Smart Control Flex Home',\n",
    " 'K 7 Power Flex',\n",
    " 'K 7 Power Flex Home',\n",
    " 'K 7 Smart Control Flex',\n",
    " 'K 7 WCM Premium Home',\n",
    " 'K 7 WCM',\n",
    " 'K 7 WCM Car&Home',\n",
    " 'K 7 WCM FJ',\n",
    " 'K 7 WCM Premium',\n",
    " 'K 5 Premium Smart Control Flex eco!Booster',\n",
    " 'K 5 Power Control Flex',\n",
    " 'K 5 Power Control Flex Home',\n",
    " 'K 5 WCM',\n",
    " 'K 5 WCM Premium',\n",
    " 'K 5 WCM Premium Home',\n",
    " 'K 5 Classic',\n",
    " 'K 5 Classic Car & Home',\n",
    " 'K 5 Classic Home',\n",
    " 'K 5 FJ',\n",
    " 'K 5 FJ Home',\n",
    " 'K 5 Premium Smart Control Flex Home',\n",
    " 'K 5 Power Control Flex Home&Brush Anniversary Edition',\n",
    " 'K 5 Premium Power Control Flex',\n",
    " 'K 5 Premium Power Control Flex Home',\n",
    " 'K 5 Premium Smart Control Flex',\n",
    " 'K 5 Smart Control Flex eco!Booster',\n",
    " 'K 4 Premium Power Control Flex',\n",
    " 'K 4 FJ',\n",
    " 'K 4 FJ Home',\n",
    " 'K 4 WCM Premium',\n",
    " 'K 4 Classic',\n",
    " 'K 4 Classic Car',\n",
    " 'K 4 Classic Home',\n",
    " 'K 4 WCM Premium Home',\n",
    " 'K Silent Anniversary Edition',\n",
    " 'K Silent eco!Booster',\n",
    " 'K 4 Power Control Flex',\n",
    " 'K 4 Power Control Flex Home',\n",
    " 'K 4 Premium Power Control Flex Home',\n",
    " 'K 4 WCM',\n",
    " 'K 3 Power Control',\n",
    " 'K 3 Classic',\n",
    " 'K 3 Classic Car',\n",
    " 'K 3 FJ',\n",
    " 'K 3 FJ Home',\n",
    " 'K 3 Horizontal Plus Home',\n",
    " 'K 3 Power Control Home T 5',\n",
    " 'K 3 Premium Power Control',\n",
    " 'K 3 Horizontal Plus',\n",
    " 'K 2 Premium FJ',\n",
    " 'K 2 Battery',\n",
    " 'K 2 Classic',\n",
    " 'K 2 Power Control',\n",
    " 'K 2 Power Control Car & Home',\n",
    " 'K 2 Premium FJ Home',\n",
    " 'K 2 Premium Horizontal VPS Home',\n",
    " 'K 2 Battery Set',\n",
    " 'K 2 Horizontal VPS',\n",
    " 'K 2 Power Control Home',\n",
    " 'K 2 Universal Edition',\n",
    " 'K 2 Universal Edition Home',\n",
    " 'K Mini'\n",
    "]\n",
    "\n",
    "\n",
    "def clean_content(text):\n",
    "\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Step 1: Temporarily replace product names with placeholders\n",
    "    placeholders = {}\n",
    "    for i, name in enumerate(product_names):\n",
    "        placeholder = f\"__PRODUCT_{i}__\"\n",
    "        placeholders[placeholder] = name\n",
    "        text = text.replace(name, placeholder)\n",
    "\n",
    "    # Step 2: Clean the rest of the text (lowercase, remove links, etc.)\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    # Step 3: Restore product names with original casing\n",
    "    for placeholder, name in placeholders.items():\n",
    "        text = text.replace(placeholder, name)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f46ec9-60f2-4ea2-8d54-f039b12f1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_content to each document's page_content\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_content(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7adcf9-79ff-4a3d-860d-5270346af28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c9a4a-7fdc-4897-bab9-6cdaa5beb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "\n",
    "embed_params = {}\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url= WX_API_URL,\n",
    "    project_id=PROJECT_ID,\n",
    "    apikey=WX_API_KEY,\n",
    "    params=embed_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864b696-8dce-44b0-ad98-8bc9a6f1c898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
